Written by:

  Stuart Chester
  
  Jason Graham <jgraha8@gmail.com>

================================================================================
				   DISCLAIMER
================================================================================

Please keep in mind that many parts of the code were developed without a clear
idea of exactly where the program was headed, since various ideas are tried and
discarded as part of the research process.  This means that code organization is
not optimal in some spots, and that not all options are fully implemented.  My
advice is ALWAYS read whatever parts of the code you are using to find out what
they really do.

================================================================================
				    OVERVIEW
================================================================================

The code lesgo solves the filtered N-S equations in the high-Re limit on a
cartisian mesh. Originally lesgo was designed to simulate flow in the
atmospheric boundary layer but has been extended and used to simulate flow over
tree canopies, wall mounted cubes, cylinders in free stream turbulence, wind
turbine arrays, and others. At its core is the LES flow solver. Built on top of
the solver are modules that provide additional functionality such as an immersed
boundary method, wind farm modeling, and others. The following is a list of the
major components of lesgo:

* LES Flow Solver

* Level Set Immersed Boundary Method

* Renormalized Numerical Simulation 

* Wind farm modeling 

* Concurrent precursor simulation

Each of these components are discussed in detail in the following sections.

================================================================================
				  DEPENDENCIES
================================================================================

The following dependencies are required in order to compile lesgo:

* Fortran 95 compiler: 

  Most modern compilers will work fine. Ones that have been used successfully
  are G95, GFortran (>= 4.4), Intel, and PGI

* makedepf90 (http://www.helsinki.fi/~eedelman/makedepf90.html) 

  The Makefile makes use of this to determine file dependencies.

* fpx3 (http://wwwuser.gwdg.de/~jbehren/fpx3)

  This is a fortran preprocessor. The main reason this is used is to isolate
  calls to MPI libraries, so that if someone wants to run the code in non-MPI
  mode, then it will compile cleanly and the compiler/linker will not complain
  about missing MPI libraries. fpx3 macros are also used to hide some other
  definitions from the compiler, e.g., some array dimensions. The Makefile runs
  the preprocessor as a step in compilation.

* tecryte >= 2.1.1 (http://panoramix.me.jhu.edu/~jgraham/codes/tecryte)

  Currently all of the data output is written in Tecplot format. tecryte is a
  library that we have developed to write data to Tecplot formatted files. All
  of the data is written in block format and has been optimized to work well
  over both local and distributed file systems.

--------------------------------------------------------------------------------
				    OPTIONAL
--------------------------------------------------------------------------------

* MPI implementation:

  A MPI implementation will be required for MPI support. The code has been
  developed and used successfully with MPICH2
  (http://www.mcs.anl.gov/research/projects/mpich2/) but others such as OpenMPI
  work as well.

* lesgo-tools (http://panoramix.me.jhu.edu/~jgraham/codes/lesgo/tools)

  lesgo-tools provides a set of scripts that aid in working with stable
  snapshots of lesgo and post processing data.

================================================================================
				GETTING STARTED
================================================================================

To get started, you'll need to configure two files: "Makefile.in" and
"lesgo.conf". The file "Makefile.in", in addition to compiler settings, has
preprocessor flags for compiling in different functionality (MPI support,
immersed boundary support, etc.). The file "lesgo.conf" is the input file for
the code which you'll set your case up. It is commented fairly well and is
hopefully self-descriptive enough to get started.

Once you configure "Makefile.in" for your system compile lesgo using:

  make

or

  make -jN

for building using N threads.

The resulting executable name will reflect the functionality that you have
configured into the executable. For instance, for serial lesgo with no addtional
bells and whistles the executable will be called 'lesgo'; if you turned on MPI
support it will be called 'lesgo-mpi'. Once you have an executable there is no
need to recompile for running different cases--this is done using "lesgo.conf".

To configure your case simply update "lesgo.conf" appropriately. As mentioned
above, "lesgo.conf" should be commented well enough to get started. If anything
is unclear, feel free to contact one of the authors listed at the top of the
file.

When running, lesgo will output data to the directory "output". Also during
execution, lesgo writes information to screen based on the value "wbase" in
"lesgo.conf". This information includes time step information, CPU time,
velocity divergence, etc. and is helpful for monitoring the case. Another
important file for monitoring the simulation is the file "output/check_ke.out"
which has the average kinetic energy of the field. Typically the kinetic energy
will decay during the spinup period and asymptote once stationarity is
reached. This gives a good indicator for determining when to start taking
statistics.

================================================================================
				LES FLOW SOLVER
================================================================================

The LES flow solver is a psuedo-spectral solver in which spectral discretization
is applied in the longitudinal and spanwise directions (x and y, repsectively)
and in the vertical (z) direction 2nd order finite differences are applied. The
solver solves the filtered N-S equations in the high-Re limit in which molecular
viscosity effects are generally neglected. An eddy viscosity closure model is
applied for the subgrid scale stresses. Boundary conditions along the x-y
perimeter are periodic where a log-law stress is applied at the bottom surface;
a stress free condition is applied at the top (channel center) boundary. Solid
objects are represented using a level set immersed boundary method. In addition
to the bottom wall, solid surfaces represented by the immersed boundary method
also apply a log-law stress boundary condition. For temporal integration the
explict 2nd order Adams-Bashforth method is used. Time stepping occurs using
either a static time step or dynamic CFL based dynamic time step. Continuity is
preserved by solving the pressure poisson equation using a direct TDMA solver.

--------------------------------------------------------------------------------
			    MPI DOMAIN DECOMPOSITION
--------------------------------------------------------------------------------

The LES flow solver is parallelized with MPI. The flow domain is evenly divided
in the vertical (z) direction between the MPI processes. Each MPI-process has
its "own" z-levels indexed by k = 1 to nz-1. The z-levels k = 0 and k = nz are
overlap nodes and are only for holding data that has been copied from the
process below or above the current one. For the main flow solver, the only
overlap data that is required is for the vertical derivatives. Since the
spectral discretization occurs along the slab planes no additional overlap data
is required. In the future, it may be advantageous to also decompose the slabs
along the y-direction and utilize the parallel FFTW3 MPI library in order to
increase scalability for large domains. 

For the pressure solver a pipelining technique was chosen to parallelize the
TDMA, since it is simple and there isn't really any better options for
parallelizing the solution of many small tridiagonal systems. The best size of
the chunks to send "down the pipeline" can be controlled via the variable
"chunksize", and WILL depend on the computer hardware and simulation size. It
would probably advantageous to add a routine that adjusts the chunk size
automatically by making speed measurements at the beginning of the simulation.

--------------------------------------------------------------------------------
			       VARIABLE LOCATIONS
--------------------------------------------------------------------------------

The code employs a staggered grid along the vertical direction. This means that
not all variables are stored on the same grid. We call the two grids the 'uv'
grid and the 'w' grid, where the grids are distinguished based on the location
of the velocity components. The only difference is that the 'uv' grid is shift
+dz/2 in the vertical direction, where dz is the vertical grid spacing. Also,
the 'w' grid conforms to the physical domain such that the first grid point lies
at z=0 and the last grid point at z=L_z where L_z is the domain height.

The following is a list of the variables and the grid they exist on. The names
are the same as found in lesgo.

        Variables			'uv' grid	'w' grid
 =======================                =========       =========
 u, v, p				    X
 w							    X
 dudx, dudy, dvdx, dvdy			    X
 dudz, dvdz                                                 X
 dwdx, dwdy 						    X
 dwdz  					    X
 dpdx, dpdy				    X
 dpdz  					                    X
 txx, txy, tyy, tzz			    X
 txz, tyz						    X
 RHSx, RHSy				    X
 RHSz							    X
 divtx, divty                               X
 divtz							    X

Important to note that when all 'uv' grid variables are written to output files
for plotting they are interpolated to the 'w' grid. This step is also performed
when time averaging variables.

================================================================================
				  DATA OUTPUT
================================================================================

File ID reservations:

  Time signal data should be written using the tecryte library. The library
  provides a mechanism for handling the file identifiers for files that will
  remain open during the simulation. It simply provides the file identifier on a
  first come first serve bases starting with 1001. So if any file identifiers
  are to be used that are not provided by the tecryte library they should be
  <=1000.
 
================================================================================
		       LEVEL SET IMMERSED BOUNDARY METHOD
================================================================================

For representing solid objects in the flow domain, the level set immersed
boundary method is used. All objects are represeted using a level set or signed
distance function (phi) where phi = 0 on the surface, phi < 0 inside objects and
phi > 0 otherwise. For each grid point in the computational domain the minimum
distance to any one of the objects in the domain must be computed and assigned
to the variable phi using a preprocessing program. The code will then use phi to
compute the surface normals which is then used to apply the log-law stress
within a small "band" along a the surface boundary. For all grid points on or
inside of the surface the velocity is forced to zero using a direct forcing
approach.

To use the level set modules in a simulation, a file "phi.out" containing the
signed distance function data must be created first.  This must be generated
with a separate program (see the program in "trees_pre_ls.f90" or
"cyl_skew_pre_ls.f90" for making the level set files). It is recommended making
the signed distance function exact if possible, or at least sampled at a higher
resolution than the computational grid.  There are many adjustable parameters
within the level set module that control how the boundary conditions at the
level set surface are applied. These are listed in the input file lesgo.conf
within the LEVEL_SET block. If you encounter problems (e.g., kinks in velocity
profiles), try adjusting some of these (e.g., length scale parameters that
control how close a given point must be to the surface before an certain action
is taken). Just make sure to read all code associated with some of these
parameters, since some are experimental, and may not be ready for "prime time".

The variables "nphitop", "nphibot", etc., control how many extra z-levels are
copied between MPI processes when determining boundary conditions (at top and
bottom of the process-local domain).  The values to use here are geometry
dependent, and it is pretty hard to determine in advance exactly what they
should be. These values should be as small as possible so the MPI transfers
involve the least amount of data. Right now, my approach has been to pick some
initial values and when the code fails, then increase the values.  the good news
here is that all the code using these is manually bounds-checked, so if an
out-of-bounds reference is made, the code should always die with an error
message saying which parameter is the problem and offer a suggestion as to what
a better value would be. To adjust these parameters add to the LEVEL_SET input
block a line that has the variable that needs to be modified. For example, if
you run into an error regarding nphitop, add to the LEVEL_SET block a line such
as:

  nphitop = 3

These are omitted by default from lesgo.conf since the defaults seem to work in
general.

Another weakness is that the number of extra z-levels is the same for all
processes, when for maximum efficiency, they really should be allowed to differ.
It should be possible to have the code auto-dimension the "nphitop", etc., for
each process.  On the other hand, this means that those processes that finish
their boundary conditions faster that the other processes will have to wait, so
unless they are given something useful to do, it is probably not worth the
effort.

--------------------------------------------------------------------------------
		       CREATING TREES WITH "trees_pre_ls"
--------------------------------------------------------------------------------

The program "trees_pre_ls" may be used for generating the "phi" field for the
level set module. This is a legacy program that generates fractal trees with
either round or square branches. It was originally created to also generate
information for RNS simulations. This feature in the trees module however has
been deprecated and is now replaced by the RNS module. So all of the trees
modules are retained solely for generated the "phi" using "trees_pre_ls".

In addtion to trees, it may also be used to easily generate an array of wall
mounted cubes which is commonly used as one of the test cases for lesgo (see
directory "utils/cubes-process-data").

To use this program, you'll first need to compile it. This is done by
configuring "Makefile.in" and "lesgo.conf" as mentioned in the GETTING STARTED
section. There are a couple settings in "trees_pre_ls.f90" that may have to be
adjusted. Now just compile "trees_pre_ls" using:

  make -f Makefile.trees_pre_ls

This should generate the program "trees_pre_ls". 

To do a simulation using the trees, the user has to write a "trees.conf"
file. The file "trees.conf" is the input file for trees_pre_ls ("lesgo.conf" is
also required by trees_pre_ls). An example "trees.conf" is:

--------------begin trees.conf-----------
# This is a trees configuration file
# this is a comment, it begins with a #

n_tree = 1

# see trees_setup_ls.f90 to see what each of these parameters does
tree = {
  n_gen = 2
  n_sub_branch = 4
  l = 0.3125
  d = 0.125
  #x0 = 0.5, 0.5, 0.0078125
  x0 = 0.5, 0.5, 0.0
  taper = 0.
  ratio = 0.48
  rel_dir = -0.4924038763,-0.8528685321,-0.1736481773,
0.0000000000,0.0000000000,1.0000000000,
-0.4924038763,0.8528685320,-0.1736481773,
0.9848077530,0.0000000000,-0.1736481773
  root_height = 0.75, 1.0, 0.75, 0.75
  twist = -90.0, 0.0, -90.0, -90.0
  #trunk_twist = 90.0
  max_res_gen=1
}
--------------end trees.conf--------------

This creates a tree structure with 2 generations of branches (the trunk counts
as generation zero), with each branch having 4 sub-branches.  The length of each
the trunk is 0.3125 and the diameter of the trunk is 0.125.  If the add_cap
option is true, then the actual length of the tree trunk will be l + d/2 =
0.3125 + 0.125/2 = 0.375.  The center of the base of the trunk is at x0 (given
as x,y,z coordinates).  The branches are not tapered.  The ratio of lengths
between a branch and each of its sub-branches is 0.48.  The directions of the
sub-branches, relative to the parent-branch coordinate system are sub-branch 1:
(-0.49, -0.85, -.17) sub-branch 2: (0, 0, 1) sub-branch 3: (-0.49, 0.85, -0.17)
sub-branch 4: (0.98, 0, -0.17) To see how the branch-local coordinate systems
are defined, see trees_setup_ls.f90. Three sub-branches are placed 75 % of the
way along the parent branch (root_height), and one is at the top of the parent
branch.  Each sub-branch has a twist about its own branch axis applied to it.
The trunk can be twisted separately, but this line has been commented out.  The
maximum resolved generation(max_res_gen) used in RNS is generation 1, note that
there n_gen = 2 is one more than the last resolved generation.  The is mean that
generation two are the unresolved RNS branches.  Even if you want to simulate
more that one unresolved branch generation, right now the code expects
max_res_gen + 1 = n_gen.  Note that although the relations between a branch and
its sub-branches are defined in trees.conf, iterated function systems (IFS) ARE
NOT USED to describe the trees.  Instead, a sub-branch is defined only by
reference to its parent branch.

Next, run "trees_pre_ls" and the level set function required by the level set
routines will be calculated. After running "trees_pre_ls", you should be all set
to begin an LES computation.
 
================================================================================
		       RENORMALIZED NUMERICAL SIMULATION
================================================================================


================================================================================
			CONCURRENT PRECURSOR SIMULATION
================================================================================

